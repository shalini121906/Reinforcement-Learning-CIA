{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SAEQtctCl1-m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid initialization\n",
        "grid_size = 100\n",
        "grid = np.zeros((grid_size, grid_size))\n"
      ],
      "metadata": {
        "id": "3y4cHWL9l9U1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obstacles placement\n",
        "num_obstacles = 2000\n",
        "obstacle_positions = random.sample(range(grid_size * grid_size), num_obstacles)\n",
        "for pos in obstacle_positions:\n",
        "    grid[pos // grid_size, pos % grid_size] = -1\n"
      ],
      "metadata": {
        "id": "FR212BAcmBuU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start and goal points\n",
        "start = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))\n",
        "goal = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))\n"
      ],
      "metadata": {
        "id": "-8Qy5XaDmC8t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MDP components\n",
        "actions = [(0, 1), (1, 0), (0, -1), (-1, 0), (1, 1), (-1, -1), (1, -1), (-1, 1)]\n",
        "rewards = np.zeros((grid_size, grid_size))\n",
        "rewards[goal] = 100\n",
        "rewards[grid == -1] = -100\n"
      ],
      "metadata": {
        "id": "lO6fF1UwmEQD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration\n",
        "def value_iteration(grid, rewards, actions, gamma=0.9, theta=1e-4):\n",
        "    V = np.zeros((grid_size, grid_size))\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                v = V[i, j]\n",
        "                action_values = []\n",
        "                for a in actions:\n",
        "                    ni, nj = i + a[0], j + a[1]\n",
        "                    if 0 <= ni < grid_size and 0 <= nj < grid_size:\n",
        "                        action_values.append(rewards[ni, nj] + gamma * V[ni, nj])\n",
        "                    else:\n",
        "                        action_values.append(-np.inf)  # Penalize invalid moves\n",
        "                V[i, j] = max(action_values)\n",
        "                delta = max(delta, abs(v - V[i, j]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            action_values = []\n",
        "            for a in actions:\n",
        "                ni, nj = i + a[0], j + a[1]\n",
        "                if 0 <= ni < grid_size and 0 <= nj < grid_size:\n",
        "                    action_values.append(rewards[ni, nj] + gamma * V[ni, nj])\n",
        "                else:\n",
        "                    action_values.append(-np.inf)  # Penalize invalid moves\n",
        "            policy[i, j] = np.argmax(action_values)\n",
        "    return V, policy"
      ],
      "metadata": {
        "id": "yzOrP9ehmFlG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning\n",
        "def q_learning(grid, rewards, actions, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
        "    Q = np.zeros((grid_size, grid_size, len(actions)))\n",
        "    for episode in range(episodes):\n",
        "        state = start\n",
        "        while state != goal:\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(range(len(actions)))\n",
        "            else:\n",
        "                action = np.argmax(Q[state[0], state[1]])\n",
        "            next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
        "            if 0 <= next_state[0] < grid_size and 0 <= next_state[1] < grid_size:\n",
        "                reward = rewards[next_state[0], next_state[1]]\n",
        "                Q[state[0], state[1], action] += alpha * (reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action])\n",
        "                state = next_state\n",
        "            else:\n",
        "                break  # Stop if the next state is out of bounds\n",
        "    policy = np.argmax(Q, axis=2)\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "Duy91vBUmG9f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_vi, policy_vi = value_iteration(grid, rewards, actions)\n",
        "Q_ql, policy_ql = q_learning(grid, rewards, actions)\n"
      ],
      "metadata": {
        "id": "kJPPphWSmIwf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "JcLYy4NVmKPX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to simulate the path taken by a policy\n",
        "def simulate_path(grid, policy, start, goal):\n",
        "    state = start\n",
        "    path = [state]\n",
        "    steps = 0\n",
        "    while state != goal:\n",
        "        action = policy[state[0], state[1]]\n",
        "        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
        "        if 0 <= next_state[0] < grid_size and 0 <= next_state[1] < grid_size:\n",
        "            state = next_state\n",
        "            path.append(state)\n",
        "            steps += 1\n",
        "        else:\n",
        "            break  # Stop if the next state is out of bounds\n",
        "    return path, steps\n"
      ],
      "metadata": {
        "id": "mRBbN9fsmPqz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate paths for each method\n",
        "path_vi, steps_vi = simulate_path(grid, policy_vi, start, goal)\n",
        "path_ql, steps_ql = simulate_path(grid, policy_ql, start, goal)\n"
      ],
      "metadata": {
        "id": "xe-L1uwxmSGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize paths\n",
        "def plot_path(grid, path, title):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(grid, cmap='gray')\n",
        "    path_x, path_y = zip(*path)\n",
        "    plt.plot(path_y, path_x, marker='o', markersize=3, color='red')\n",
        "    plt.scatter(start[1], start[0], marker='*', color='green', s=200)\n",
        "    plt.scatter(goal[1], goal[0], marker='*', color='blue', s=200)\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "O-OMU0PvmUTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_path(grid, path_vi, 'Value Iteration Path')\n",
        "plot_path(grid, path_ql, 'Q-Learning Path')\n"
      ],
      "metadata": {
        "id": "N8VJJJsGmXoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Value Iteration Steps: {steps_vi}\")\n",
        "print(f\"Q-Learning Steps: {steps_ql}\")\n"
      ],
      "metadata": {
        "id": "YDFoGM5OmZhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZrW7qvvmhbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}